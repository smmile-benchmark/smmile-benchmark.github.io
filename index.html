<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="description" content="SMMILE: The first expert-driven multimodal in-context learning benchmark for medical tasks, developed by an international team of 11 medical experts.">
  <meta property="og:title" content="SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning"/>
  <meta property="og:description" content="First expert-driven multimodal ICL benchmark for medical tasks with 111 problems across 6 specialties and 13 imaging modalities"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/image/smmile_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="SMMILE: Expert-Driven Medical In-Context Learning Benchmark">
  <meta name="twitter:description" content="First multimodal ICL benchmark for medical tasks with expert-curated problems">
  <meta name="twitter:image" content="static/images/smmile_twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="multimodal, in-context learning, medical AI, benchmark, MLLM, medical imaging, expert-driven">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning</h1>
          <div style="display: flex; justify-content: center; gap: 30px; margin-bottom: 15px; align-items: center;">
            <img src="static/images/eth.png" alt="ETH Zurich Logo" style="height: 60px;">
            <img src="static/images/stanford-logo.png" alt="Stanford University Logo" style="height: 60px;">
          </div>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Melanie Rieff</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Maya Varma</a><sup>2*</sup>,</span>
                <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ossian Rabow</a><sup>2,3</sup>,</span>
                <span class="author-block">
                  <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Subathra Adithan</a><sup>4</sup>,</span>
                <span class="author-block">
                  <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Julie Kim</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="SIXTH AUTHOR PERSONAL LINK" target="_blank">Ken Chang</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="SEVENTH AUTHOR PERSONAL LINK" target="_blank">Hannah Lee</a><sup>5</sup>,</span>
                <span class="author-block">
                  <a href="EIGHTH AUTHOR PERSONAL LINK" target="_blank">Nidhi Rohatgi</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="NINTH AUTHOR PERSONAL LINK" target="_blank">Christian Bluethgen</a><sup>2,6,7</sup>,</span>
                <span class="author-block">
                  <a href="TENTH AUTHOR PERSONAL LINK" target="_blank">Mohamed S. Muneer</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="ELEVENTH AUTHOR PERSONAL LINK" target="_blank">Jean-Benoit Delbrouck</a><sup>2,8†</sup>,</span>
                <span class="author-block">
                  <a href="TWELFTH AUTHOR PERSONAL LINK" target="_blank">Michael Moor</a><sup>1†</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ETH Zurich</span>
              <span class="author-block"><sup>2</sup>Stanford University</span>
              <span class="author-block"><sup>3</sup>Lund University</span><br>
              <span class="author-block"><sup>4</sup>Jawaharlal Institute of Postgraduate Medical Education and Research</span>
              <span class="author-block"><sup>5</sup>UCSF</span><br>
              <span class="author-block"><sup>6</sup>University of Zurich</span>
              <span class="author-block"><sup>7</sup>University Hospital Zurich</span>
              <span class="author-block"><sup>8</sup>HOPPR</span><br>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution <sup>†</sup>Co-senior authors</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/YOUR REPO HERE" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <!-- HuggingFace Dataset link -->
          <span class="link-block">
            <a href="https://huggingface.co/smmile" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-database"></i>
            </span>
            <span>Dataset</span>
          </a>
        </span>

          <!-- ArXiv abstract Link -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-justified">
        <p>
          Multimodal in-context learning (ICL) remains underexplored despite significant potential for specialized domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts contributed problems, each including (1) a multimodal query and (2) multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. Benchmark evaluation of 15 MLLMs reveals that most exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. Analysis reveals the importance of selecting relevant in-context examples: one noisy or irrelevant example can reduce performance by up to 8%. We identify recency bias in MLLMs, where placing the most relevant example last boosts accuracy by up to 71%. SMMILE thus highlights critical limitations and biases in current MLLMs when learning multimodal medical tasks from context.
        </p>
      </div>
    </div>
  </div>
</div>
</section>
<!-- End paper abstract -->

<!-- How is SMMILE created section -->
<section class="section">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">How is SMMILE created?</h2>
      <div class="content has-text-justified">
        <p>
          SMMILE was developed through a collaborative effort with an international team of 11 medical experts from leading institutions. Our expert-driven approach ensures clinical relevance and accuracy of the benchmark problems.
        </p>
        <p>
          <strong>Expert Collaboration:</strong> We recruited clinical domain experts including 9 medical doctors and 2 medical students with an average of 6.4 years of clinical experience. The experts have specialty expertise in radiology, general medicine, and pathology.
        </p>
        <p>
          <strong>Problem Creation Process:</strong> Each expert contributed 10 problems through a guided, step-by-step web interface. The problems span 6 medical specialties and 13 imaging modalities, ensuring comprehensive coverage of medical visual reasoning tasks.
        </p>
        <p>
          <strong>Quality Control:</strong> Every problem underwent rigorous manual quality control, including inspection by multiple authors, categorization, and spell-checking. This resulted in a high-quality dataset with consistent formatting and clinical accuracy.
        </p>
      </div>
    </div>
  </div>
</div>
</section>

<!-- Examples of Problems section -->
<section class="section hero is-light">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Examples of Problems</h2>
      <div class="content has-text-justified">
        <p>
          SMMILE problems consist of multimodal in-context examples followed by a query that tests the model's ability to learn from the provided demonstrations. Each problem includes:
        </p>
        <ul>
          <li><strong>In-context Examples:</strong> 2-19 expert-curated image-question-answer triplets that demonstrate the task</li>
          <li><strong>Query:</strong> A final image-question pair that the model must answer based on the learned pattern</li>
          <li><strong>Ground Truth:</strong> Expert-validated answers for evaluation</li>
        </ul>
        <p>
          <strong>Medical Specialties Covered:</strong> Radiology, Pathology, Dermatology, Ophthalmology, Surgery, and General Medicine
        </p>
        <p>
          <strong>Imaging Modalities:</strong> X-Ray, CT, MRI, Ultrasound, Photographs, Staining, ECG, EEG, Mammogram, Fundus Photography, and more
        </p>
        <p>
          <strong>Task Types:</strong> Classification problems, diagnostic questions, reasoning tasks, and quantitative analysis requiring various cognitive processes from pattern recognition to complex medical reasoning.
        </p>
      </div>
    </div>
  </div>
</div>
</section>

<!-- Results section -->
<section class="section">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">
        <p>
          We evaluated 15 state-of-the-art multimodal large language models on SMMILE, including both open-source and closed-source models. Our findings reveal significant limitations in current MLLMs' ability to perform multimodal in-context learning in medical settings.
        </p>

        <h4 class="title is-4">Key Findings</h4>
        <p>
          <strong>Limited ICL Benefits:</strong> Most MLLMs show minimal improvement from in-context learning, with only an average 6.1-point improvement on SMMILE and 7.3 points on SMMILE++. Even the best-performing models (GPT-4o and Qwen2.5-VL-72B) achieve only around 50% accuracy in open-ended evaluations.
        </p>
        <p>
          <strong>Model Performance:</strong> GPT-4o emerged as the overall leader on SMMILE, while Qwen2.5-VL-72B showed superior performance on the larger SMMILE++ dataset. Domain-specific medical models did not consistently outperform general-purpose models of similar size.
        </p>
        <p>
          <strong>Critical Biases Identified:</strong> We discovered important limitations including recency bias (models heavily weight the last example, with up to 71-point accuracy differences) and sensitivity to example quality (one irrelevant example can reduce performance by up to 8 points).
        </p>
        <p>
          <strong>Task-Specific Challenges:</strong> Models particularly struggled with numerical answers (achieving 0% accuracy), quantitative reasoning tasks, and certain imaging modalities like MRI and medical illustrations.
        </p>
        <p>
          These results highlight the substantial gap between current MLLM capabilities and the requirements for reliable clinical applications, pointing to important directions for future model development.
        </p>
      </div>
    </div>
  </div>
</div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
<div class="hero-body">
  <div class="container">
    <div id="results-carousel" class="carousel results-carousel">
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/smmile_overview.jpg" alt="SMMILE benchmark overview"/>
      <h2 class="subtitle has-text-centered">
        Expert-driven multimodal in-context learning problems across 6 medical specialties.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/dataset_stats.jpg" alt="Dataset statistics"/>
      <h2 class="subtitle has-text-centered">
        Comprehensive analysis of 111 problems covering diverse medical imaging modalities.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/performance_results.jpg" alt="Performance results"/>
      <h2 class="subtitle has-text-centered">
       Evaluation results showing limited ICL benefits across 15 state-of-the-art MLLMs.
     </h2>
   </div>
   <div class="item">
    <!-- Your image here -->
    <img src="static/images/bias_analysis.jpg" alt="Bias analysis"/>
    <h2 class="subtitle has-text-centered">
      Analysis revealing recency bias and importance of example quality in MLLMs.
    </h2>
  </div>
</div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{rieff2025smmile,
  title={SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning},
  author={Rieff, Melanie and Varma, Maya and Rabow, Ossian and Adithan, Subathra and Kim, Julie and Chang, Ken and Lee, Hannah and Rohatgi, Nidhi and Bluethgen, Christian and Muneer, Mohamed S. and Delbrouck, Jean-Benoit and Moor, Michael},
  year={2025},
  journal={arXiv preprint},
  url={https://arxiv.org/abs/2506.06091},
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->

</body>
</html>