<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SMMILE: The first expert-driven multimodal in-context learning benchmark for medical tasks, developed by an international team of 11 medical experts.">
  <meta property="og:title" content="SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning"/>
  <meta property="og:description" content="First expert-driven multimodal ICL benchmark for medical tasks with 111 problems across 6 specialties and 13 imaging modalities"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/image/smmile_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="SMMILE: Expert-Driven Medical In-Context Learning Benchmark">
  <meta name="twitter:description" content="First multimodal ICL benchmark for medical tasks with expert-curated problems">
  <meta name="twitter:image" content="static/images/smmile_twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="multimodal, in-context learning, medical AI, benchmark, MLLM, medical imaging, expert-driven">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/smmile_favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div style="display: flex; justify-content: center; margin-bottom: 20px;">
            <img src="static/images/logo.pdf" alt="Organization Logo" style="height: 250px;">
          </div>
          <h1 class="title is-1 publication-title">SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning</h1>
          <div style="display: flex; justify-content: center; gap: 30px; margin-bottom: 15px; align-items: center;">
            <img src="static/images/eth.png" alt="ETH Zurich Logo" style="height: 60px;">
            <img src="static/images/stanford-logo.png" alt="Stanford University Logo" style="height: 60px;">
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/melanie-r-3296aa268/" target="_blank">Melanie Rieff</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://maya-varma.com" target="_blank">Maya Varma</a><sup>2*</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/ossian-rabow-0ba918241/?originalSubdomain=se" target="_blank">Ossian Rabow</a><sup>2,3</sup>,</span>
                <span class="author-block">
                  <a href="https://subathra.info" target="_blank">Subathra Adithan</a><sup>4</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/juliejaekim/" target="_blank">Julie Kim</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/ken-chang-4ab12023/" target="_blank">Ken Chang</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://pathology.ucsf.edu/about/faculty/hannah-lee" target="_blank">Hannah Lee</a><sup>5</sup>,</span>
                <span class="author-block">
                  <a href="https://profiles.stanford.edu/nidhi-rohatgi" target="_blank">Nidhi Rohatgi</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/bluethgen/" target="_blank">Christian Bluethgen</a><sup>2,6,7</sup>,</span>
                <span class="author-block">
                  <a href="https://med.stanford.edu/xray/CR/MohamedMuneerMD.html" target="_blank">Mohamed S. Muneer</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://jbdel.github.io" target="_blank">Jean-Benoit Delbrouck</a><sup>2,8‚Ä†</sup>,</span>
                <span class="author-block">
                  <a href="https://michaelmoor.me" target="_blank">Michael Moor</a><sup>1‚Ä†</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ETH Zurich</span>
              <span class="author-block"><sup>2</sup>Stanford University</span>
              <span class="author-block"><sup>3</sup>Lund University</span><br>
              <span class="author-block"><sup>4</sup>Jawaharlal Institute of Postgraduate Medical Education and Research</span>
              <span class="author-block"><sup>5</sup>UCSF</span><br>
              <span class="author-block"><sup>6</sup>University of Zurich</span>
              <span class="author-block"><sup>7</sup>University Hospital Zurich</span>
              <span class="author-block"><sup>8</sup>HOPPR</span><br>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution <sup>‚Ä†</sup>Co-senior authors</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
            <span class="link-block">
              <span class="external-link button is-normal is-rounded is-dark" style="cursor: not-allowed; opacity: 0.6;">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code (Coming Soon)</span>
            </span>
          </span>

          <span class="link-block">
            <a href="https://huggingface.co/smmile" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-database"></i>
            </span>
            <span>Dataset</span>
          </a>
        </span>

          <span class="link-block">
            <a href="https://arxiv.org/abs/2506.21355" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</div>
</section>

<section class="section hero is-light">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-justified">
        <p>
          Multimodal in-context learning (ICL) remains underexplored despite significant potential for specialized domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts contributed problems, each including (1) a multimodal query and (2) multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. Benchmark evaluation of 15 MLLMs reveals that most exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. Analysis reveals the importance of selecting relevant in-context examples: one noisy or irrelevant example can result in average performance reductions of up to 9.5%. We also identify a recency bias in MLLMs, where placing the most relevant example last in the example list can lead to substantial improvements in performance. SMMILE thus highlights critical limitations and biases in current MLLMs when learning multimodal medical tasks from context.
        </p>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">How is SMMILE created?</h2>
      <div class="content has-text-justified">

        <div class="box">
          <h4 class="title is-5">üë• Expert Collaboration</h4>
          <b>S</b>tanford <b>M</b>ultimodal <b>M</b>edical <b>I</b>n-Context <b>Le</b>arning (SMMILE) was developed through a collaborative effort with an international team of 11 medical experts from leading institutions with an average of 6.4 years of clinical experience. The experts have specialty expertise in radiology, general medicine, and pathology. Our expert-driven approach ensures clinical relevance and accuracy of the benchmark problems.
        </div>

        <div class="box">
          <h4 class="title is-5">üîß Problem Creation Process</h4>
          Each expert contributed 10 problems through a guided, step-by-step web interface. The problems span 6 medical specialties and 13 imaging modalities, ensuring comprehensive coverage of medical visual reasoning tasks. Each problem includes multimodal in-context examples followed by a query that tests the model's ability to learn from the provided demonstrations.
        </div>

        <div class="box">
          <h4 class="title is-5">‚úÖ Quality Control</h4>
          Every problem underwent rigorous manual quality control, including inspection by multiple authors, categorization, and spell-checking. This resulted in a high-quality dataset with consistent formatting and clinical accuracy. The final dataset contains 111 problems with 517 in-context examples.
        </div>

      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-light">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Examples of Problems</h2>
      <div class="content has-text-justified">

        <div class="box">
          <h4 class="title is-5">üìã Problem Structure</h4>
          SMMILE problems consist of multimodal in-context examples followed by a query that tests the model's ability to learn from the provided demonstrations. Each problem includes:
          <ul>
            <li><strong>In-context Examples:</strong> Expert-curated in-context learning problems that demonstrate the task</li>
            <li><strong>Query:</strong> A final image-question pair that the model must answer based on the learned pattern</li>
            <li><strong>Ground Truth:</strong> Expert-validated answers for evaluation</li>
          </ul>
        </div>

        <div class="box">
          <h4 class="title is-5">üè• Medical Coverage</h4>
          <strong>Medical Specialties Covered:</strong> Radiology, Pathology, Dermatology, Ophthalmology, Surgery, and General Medicine<br><br>
          <strong>Imaging Modalities:</strong> X-Ray, CT, MRI, Ultrasound, Photographs, Staining, ECG, EEG, Mammogram, Fundus Photography, and more<br><br>
          <strong>Task Types:</strong> Classification problems, diagnostic questions, reasoning tasks, and quantitative analysis requiring various cognitive processes from pattern recognition to complex medical reasoning.
        </div>

        <div style="text-align: center; margin-top: 20px; margin-bottom: 20px;">
          <figure>
            <img src="static/images/examples_v3.png" alt="Examples of SMMILE problems" style="width: 100%; max-width: 800px;">
            <figcaption style="font-size: small; text-align: left;">
              <center>Examples of SMMILE problems showing multimodal in-context learning tasks across different medical specialties and imaging modalities.</center>
            </figcaption>
          </figure>
        </div>

      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-small">
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Results</h2>
      <div class="content has-text-justified">

        <div class="box">
          <h4 class="title is-5">üîç Evaluation Overview</h4>
          We evaluated 15 state-of-the-art multimodal large language models on SMMILE, including both open-source and closed-source models. Our findings reveal significant limitations in current MLLMs' ability to perform multimodal in-context learning in medical settings.
        </div>

        <div class="box">
          <h4 class="title is-5">üìä Key Findings</h4>
          <strong>Limited ICL Benefits:</strong> Most MLLMs show minimal improvement from in-context learning, with only an average 8% improvement on SMMILE and 9.4% on SMMILE++.<br><br>

          <strong>Model Performance:</strong> GPT-4o emerged as the overall leader on SMMILE, while Qwen2.5-VL-72B showed superior performance on the larger SMMILE++ dataset. Domain-specific medical models did not consistently outperform general-purpose models of similar size.
        </div>

        <div class="box">
          <h4 class="title is-5">‚ö†Ô∏è Critical Biases Identified</h4>
          We discovered important limitations including recency bias (models heavily weight the last example, and sensitivity to example quality (one irrelevant example can reduce performance). Models particularly struggled with numerical answers (achieving 0% accuracy), quantitative reasoning tasks, and certain imaging modalities like MRI and medical illustrations.
        </div>

        <div class="box">
          <h4 class="title is-5">üéØ Implications</h4>
          These results highlight the substantial gap between current MLLM capabilities and the requirements for reliable clinical applications, pointing to important directions for future model development in medical multimodal in-context learning.
        </div>

      </div>
    </div>
  </div>
</div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{rieff2025smmileexpertdrivenbenchmarkmultimodal,
      title={SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning},
      author={Melanie Rieff and Maya Varma and Ossian Rabow and Subathra Adithan and Julie Kim and Ken Chang and Hannah Lee and Nidhi Rohatgi and Christian Bluethgen and Mohamed S. Muneer and Jean-Benoit Delbrouck and Michael Moor},
      year={2025},
      eprint={2506.21355},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.21355},
}
    </code></pre>
  </div>
</section>

</body>
</html>